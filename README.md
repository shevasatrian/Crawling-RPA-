# Quest 1 - RPA Crawling: Auto Extract PDF from Google Scholar

Automatically searches Google Scholar, extracts paper listings, and downloads available PDFs â€” using Puppeteer with Stealth plugin to avoid bot detection.

## Requirements

- Node.js v16+
- npm
- Windows OS (as per Quest requirement)
- Internet connection

## Setup

```bash
# 1. Install dependencies
npm install

# 2. Run the crawler
npm start
```

## Usage

### Default (searches "machine learning", downloads up to 5 PDFs)
```bash
node src/index.js
```

### Custom query and limit
```bash
node src/index.js --query="deep learning" --max=3
```

### Headless mode (browser runs in background â€” faster)
```bash
node src/index.js --headless=true
```

### Full custom example
```bash
node src/index.js --query="neural network" --max=5 --headless=false
```

## How It Works

```
[1/4] Launch stealth browser
      â†“ Puppeteer + puppeteer-extra-plugin-stealth
      â†“ Realistic user-agent, hidden webdriver flags

[2/4] Search Google Scholar
      â†“ Human-like typing (random delay per keystroke)
      â†“ Random delays between actions

[3/4] Extract paper results
      â†“ Scrapes titles, authors, paper URLs
      â†“ Detects [PDF] direct links

[4/4] Download PDFs (dual strategy)
      â†“ Strategy A: Direct [PDF] link from Scholar results â†’ download immediately
      â†“ Strategy B: Visit paper page â†’ scan for PDF links â†’ download
```

## Anti-Bot Techniques Used

| Technique | Purpose |
|---|---|
| `puppeteer-extra-plugin-stealth` | Hides 20+ Puppeteer fingerprints |
| Realistic User-Agent | Mimics real Chrome browser |
| Human-like typing delay | Random 50â€“120ms per character |
| Random action delays | 300â€“800ms between interactions |
| Override `navigator.webdriver` | Hides automation flag |
| Realistic HTTP headers | Accept-Language, Accept headers |

## Output

PDFs are saved to `./output/pdfs/` with sanitized filenames.

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ“Š CRAWL SUMMARY                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Papers Found   : 10                         â•‘
â•‘  PDFs Downloaded: 4                          â•‘
â•‘  Failed/Skipped : 1                          â•‘
â•‘  Output Dir     : ./output/pdfs             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  â±  Time Elapsed: 12.43s                    â•‘
â•‘  Performance    : âœ… TARGET MET              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## Project Structure

```
quest1-crawling-rpa/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.js      # Main orchestrator
â”‚   â”œâ”€â”€ browser.js    # Stealth browser setup + helpers
â”‚   â”œâ”€â”€ scholar.js    # Google Scholar interaction
â”‚   â”œâ”€â”€ downloader.js # PDF download (direct + page scan)
â”‚   â””â”€â”€ config.js     # All settings in one place
â”œâ”€â”€ output/
â”‚   â””â”€â”€ pdfs/         # Downloaded PDFs saved here
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## Performance Notes

- **Target**: â‰¤ 16 seconds
- **Optimal**: â‰¤ 8 seconds
- Most time is spent on: network requests + Scholar page loading
- Use `--headless=true` to shave ~1â€“2 seconds off render time
- Scholar may show CAPTCHA if too many requests â€” wait 10â€“15 min and retry

## Troubleshooting

**CAPTCHA detected:**
- Wait 10â€“15 minutes before retrying
- Try a different network/VPN
- Reduce `--max` to 2â€“3 papers

**PDF not available:**
- Many papers on Scholar are behind paywalls
- Open-access papers (arXiv, ResearchGate, etc.) have the best availability
- Try `--query="arxiv machine learning"` for better PDF availability
